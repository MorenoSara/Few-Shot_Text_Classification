{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Supervised_Text_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN61/RsRxBfge3wQY+WxIYS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MorenoSara/Few-Shot_Text_Classification/blob/main/Supervised_Text_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers"
      ],
      "metadata": {
        "id": "0TocqxWRoq3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGXhqTirYuFD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "from sentence_transformers.util import cos_sim\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = pd.read_excel('train.xlsx', index_col=0) # 32889 samples\n",
        "eval_dataset = pd.read_excel('valid.xlsx', index_col=0)\n",
        "test_dataset = pd.read_excel('test.xlsx', index_col=0)"
      ],
      "metadata": {
        "id": "Iwk4xySVZChA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "REMAP_LEV1 = {'CS': 'Computer Science', \n",
        "              'Civil': 'Civil Engineering', \n",
        "              'ECE': 'Electrical Engineering', \n",
        "              'Psychology': 'Psychology', \n",
        "              'MAE': 'Mechanical Engineering', \n",
        "              'Medical': 'Medical Science', \n",
        "              'biochemistry': 'Biochemistry'}"
      ],
      "metadata": {
        "id": "07HnOFfSvpbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mapped_labels(data, mapping_dict):\n",
        "  labels = [l.strip() for l in data]\n",
        "  return list(map(lambda l: mapping_dict[l], labels))"
      ],
      "metadata": {
        "id": "imnQaupNDAAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_set = get_mapped_labels(set(train_dataset['Domain']), REMAP_LEV1)\n",
        "\n",
        "training_docs = train_dataset['Abstract']\n",
        "training_labels = get_mapped_labels(train_dataset['Domain'], REMAP_LEV1)\n",
        "\n",
        "eval_docs = eval_dataset['Abstract']\n",
        "eval_labels = get_mapped_labels(eval_dataset['Domain'], REMAP_LEV1)\n",
        "\n",
        "test_docs = test_dataset['Abstract']\n",
        "test_labels = get_mapped_labels(test_dataset['Domain'], REMAP_LEV1)\n",
        "\n",
        "print(f\"Training set: {len(training_docs)}, {len(training_labels)}\") # 32889 samples\n",
        "print(f\"Evaluation set: {len(eval_docs)}, {len(eval_labels)}\") # 4698 samples\n",
        "print(f\"Test set: {len(test_docs)}, {len(test_labels)}\") # 9398 samples"
      ],
      "metadata": {
        "id": "kNR3b_jeDbjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "integer_labels = le.fit_transform(labels_set)\n",
        "\n",
        "int_training_labels = le.transform(training_labels)\n",
        "int_eval_labels = le.transform(eval_labels)\n",
        "int_test_labels = le.transform(test_labels)\n",
        "\n",
        "ohe = OneHotEncoder(sparse=False)\n",
        "ohe.fit(integer_labels.reshape(-1,1))\n",
        "\n",
        "ohe_training_labels = ohe.transform(int_training_labels.reshape(-1,1)) # (32889, 7)\n",
        "ohe_eval_labels = ohe.transform(int_eval_labels.reshape(-1,1)) # (4698, 7)\n",
        "ohe_test_labels = ohe.transform(int_test_labels.reshape(-1,1)) # (9398, 7)"
      ],
      "metadata": {
        "id": "SiUJaeP_pTdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class document_class(Dataset):\n",
        "  def __init__(self, documents, labels):\n",
        "    self.train_df = []\n",
        "    for id, doc in enumerate(documents):\n",
        "      curr_doc = [labels[id]]\n",
        "      curr_doc.append(doc) \n",
        "      self.train_df.append(curr_doc)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    return self.train_df[index] \n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.train_df)"
      ],
      "metadata": {
        "id": "c09KfrzDnQqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def my_collate_fn(batch):\n",
        "  documents = []\n",
        "  labels = []\n",
        "  for doc in batch:\n",
        "    documents.append(doc[1])\n",
        "    labels.append(list(doc[0]))\n",
        "  return (documents, torch.Tensor(labels))"
      ],
      "metadata": {
        "id": "2N9GTKpdmPAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, model_path):\n",
        "    \"\"\"Save model.\"\"\"\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "\n",
        "def load_model(model, model_path, use_cuda=True):\n",
        "    \"\"\"Load model.\"\"\"\n",
        "    map_location = 'cpu'\n",
        "    if use_cuda and torch.cuda.is_available():\n",
        "        map_location = 'cuda:0'\n",
        "    model.load_state_dict(torch.load(model_path, map_location))\n",
        "    return model"
      ],
      "metadata": {
        "id": "Tmw_Amvr0qoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the classifier and maintain the pre-trained sentence transformer "
      ],
      "metadata": {
        "id": "pXWQa1zHCVA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class complete_model_no_st_finetuning(nn.Module):\n",
        "  def __init__(self, sentence_transformer_model, st_embedding_dimension, num_classes, device):\n",
        "    super().__init__()\n",
        "    self.st = SentenceTransformer(sentence_transformer_model)\n",
        "    self.classification = nn.Linear(in_features=st_embedding_dimension, out_features=num_classes)\n",
        "    self.device = device\n",
        "    self.to(device)\n",
        "\n",
        "  def forward(self, documents):\n",
        "    docs = self.st.encode(documents) # exploit pretrained sentence transformer\n",
        "    probs = self.classification(torch.Tensor(docs).to(self.device)) # assign a score to each class for every document\n",
        "    return probs"
      ],
      "metadata": {
        "id": "kWVcs3OwbeFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = complete_model_no_st_finetuning('sentence-transformers/all-mpnet-base-v2', 768, len(labels_set), device)"
      ],
      "metadata": {
        "id": "-egseZj-zx5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1e-2\n",
        "epochs = 5\n",
        "batch_size = 256\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, betas = [0.9, 0.999], eps=1e-8)\n",
        "\n",
        "training_documents = document_class(training_docs, ohe_training_labels)\n",
        "training_dataloader = DataLoader(training_documents, batch_size=batch_size, shuffle=True, collate_fn=my_collate_fn)\n",
        "\n",
        "eval_documents = document_class(eval_docs, ohe_eval_labels)\n",
        "eval_dataloader = DataLoader(eval_documents, batch_size=batch_size, shuffle=True, collate_fn=my_collate_fn)"
      ],
      "metadata": {
        "id": "DVUaAhm6xwz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_eval_loss = np.inf\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  training_loss = 0\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  for batch, (docs, labels) in enumerate(training_dataloader):\n",
        "\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    probabilities = model(docs)\n",
        "    loss = criterion(probabilities, labels)\n",
        "    training_loss += loss\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f'Batch: {batch}/{len(training_dataloader)}, epoch: {epoch}/{epochs}. Training loss: {training_loss:.3f}.')\n",
        "    break\n",
        "\n",
        "  model.eval()\n",
        "  eval_loss = 0\n",
        "  \n",
        "  for eval_batch, (eval_docums, eval_labels) in enumerate(eval_dataloader):\n",
        "    eval_labels = eval_labels.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      eval_probs = model(eval_docums)\n",
        "\n",
        "    batch_eval_loss = criterion(eval_probs, eval_labels)\n",
        "    eval_loss += batch_eval_loss.item()\n",
        "    print(f'Evaluation: Batch: {eval_batch}/{len(eval_dataloader)}, epoch: {epoch}/{epochs}. Training loss: {eval_loss:.3f}.')\n",
        "  \n",
        "  print(\"\\nEvaluation loss: \", eval_loss)\n",
        "  print('\\n')\n",
        "\n",
        "  if eval_loss < best_eval_loss:\n",
        "    print(\"Saving best model\")\n",
        "    best_eval_loss = eval_loss\n",
        "    save_model(model, './best_model.pkl')\n"
      ],
      "metadata": {
        "id": "f2HY59PmYsYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the classifier and finetune the sentence transformer"
      ],
      "metadata": {
        "id": "XNC6iD5rD-0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, device, model_name: str = 'sentence-transformers/all-mpnet-base-v2') -> None:\n",
        "        super(TextEncoder, self).__init__()\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModel.from_pretrained(model_name).to(device)\n",
        "\n",
        "    # def forward(self, text: Union[str, List[str]]) -> Tensor:\n",
        "    def forward(self, text) -> torch.Tensor:\n",
        "        inp = self.tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
        "        inp = inp.to(device)\n",
        "        out = self.model(**inp)[0]  # First element of model_output contains all token embeddings.\n",
        "        out = self.mean_pooling(out, inp['attention_mask'])\n",
        "        if isinstance(text, str):  # If input is just 1 string -> return 1D embeddings.\n",
        "            out = out.squeeze(0)\n",
        "        return nn.functional.normalize(out, p=2, dim=-1)\n",
        "\n",
        "    def mean_pooling(self, token_embeddings, attention_mask):\n",
        "        input_mask_expanded = \\\n",
        "            attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)"
      ],
      "metadata": {
        "id": "E5eXxRaR_6uC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class complete_model_finetuning(nn.Module):\n",
        "  def __init__(self, sentence_transformer_model, st_embedding_dimension, num_classes, device):\n",
        "    super().__init__()\n",
        "    self.st = TextEncoder(device, sentence_transformer_model)\n",
        "    self.classification = nn.Linear(in_features=st_embedding_dimension, out_features=num_classes)\n",
        "    self.device = device\n",
        "    self.to(device)\n",
        "\n",
        "  def forward(self, documents):\n",
        "    docs = self.st(documents)\n",
        "    probs = self.classification(docs) # assign a score to each class for every document\n",
        "    return probs"
      ],
      "metadata": {
        "id": "sG9OHuqBPvDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = complete_model_finetuning('sentence-transformers/all-mpnet-base-v2', 768, len(labels_set), device)"
      ],
      "metadata": {
        "id": "Zf9QD_pSEKOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1e-2\n",
        "epochs = 5\n",
        "batch_size = 16\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, betas = [0.9, 0.999], eps=1e-8)\n",
        "\n",
        "training_documents = document_class(training_docs, ohe_training_labels)\n",
        "training_dataloader = DataLoader(training_documents, batch_size=batch_size, shuffle=True, collate_fn=my_collate_fn)\n",
        "\n",
        "eval_documents = document_class(eval_docs, ohe_eval_labels)\n",
        "eval_dataloader = DataLoader(eval_documents, batch_size=batch_size, shuffle=True, collate_fn=my_collate_fn)"
      ],
      "metadata": {
        "id": "AxATc1yIFew8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_eval_loss = np.inf\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  training_loss = 0\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  for batch, (docs, labels) in enumerate(training_dataloader):\n",
        "\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    probabilities = model(docs)\n",
        "    loss = criterion(probabilities, labels)\n",
        "    training_loss += loss\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f'Batch: {batch}/{len(training_dataloader)}, epoch: {epoch}/{epochs}. Training loss: {training_loss:.3f}.')\n",
        "    break\n",
        "\n",
        "  model.eval()\n",
        "  eval_loss = 0\n",
        "\n",
        "  for eval_batch, (eval_docums, eval_labels) in enumerate(eval_dataloader):\n",
        "    eval_labels = eval_labels.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      eval_probs = model(eval_docums)\n",
        "\n",
        "    batch_eval_loss = criterion(eval_probs, eval_labels)\n",
        "    eval_loss += batch_eval_loss.item()\n",
        "    print(f'Evaluation: Batch: {eval_batch}/{len(eval_dataloader)}, epoch: {epoch}/{epochs}. Training loss: {eval_loss:.3f}.')\n",
        "  \n",
        "  print(\"\\nEvaluation loss: \", eval_loss)\n",
        "  print('\\n')\n",
        "\n",
        "  if eval_loss < best_eval_loss:\n",
        "    print(\"Saving best model\")\n",
        "    best_eval_loss = eval_loss\n",
        "    save_model(model, './finetuned_model.pkl')"
      ],
      "metadata": {
        "id": "A4o2QxxEQQEq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}